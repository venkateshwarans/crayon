---
description: Rendering markdown formatted agent responses in Crayon
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Rendering Markdown

A markdown renderer is a fundamental component that enables your Crayon Chat application to display formatted text responses from AI models. By implementing this renderer, you can:

- Display AI responses with proper formatting using Markdown
- Support rich text features like **bold**, _italic_, lists, and code blocks
- Maintain consistent text styling across your chat interface
- Handle basic text responses without needing complex custom templates

This guide demonstrates how to add a markdown renderer template to your Crayon Chat application. This assumes you have already set up your Next.js project following
the [NextJS Quickstart Guide](/quickstart/002-nextjs.mdx).

## Step 1: Install Required Dependencies

First, install the markdown renderer:

<Tabs groupId="js-package-manager">
  <TabItem value="npm" label="npm">
    ```bash npm install react-markdown ```
  </TabItem>
  <TabItem value="pnpm" label="pnpm">
    ```bash pnpm add react-markdown ```
  </TabItem>
  <TabItem value="yarn" label="yarn">
    ```bash yarn add react-markdown ```
  </TabItem>
</Tabs>

## Step 2: Create Markdown Template

Create a new file for your markdown template:

<Tabs groupId="frontend-lang">
  <TabItem value="ts" label="TypeScript">
```typescript title="app/templates/markdown.tsx"
import React from "react";
import Markdown from "react-markdown";

export const MarkdownTemplate: React.FC<{ children: React.ReactNode }> = ({
  children,
}) => {
  return <Markdown>{children as string}</Markdown>;
};
```
  </TabItem>
  <TabItem value="js" label="JavaScript">
```javascript title="app/templates/markdown.jsx"
import React from "react";
import Markdown from "react-markdown";

export const MarkdownTemplate = ({ children }) => {
  return <Markdown>{children}</Markdown>;
};
```
  </TabItem>
</Tabs>

## Step 3: Update Your Page Component

If you've followed the Quick Start guide, just add the text template to your existing templates array:

<Tabs groupId="frontend-lang">
  <TabItem value="ts" label="TypeScript">

```typescript title="app/page.tsx"
import { MarkdownTemplate } from "@/app/templates/markdown";

const YourComponent = () =>Â {
  // ... implement processMessage as shown in the 'Getting Started with Crayon' guide

  return (
    <CrayonChat
      processMessage={processMessage}  // Required for NextJS setup
      responseTemplates={[{
        name: "text",
        Component: MarkdownTemplate
      }]}
    />
  )
}
```

  </TabItem>
  <TabItem value="js" label="JavaScript">

```javascript title="app/page.jsx"
import { TextTemplate } from "@/app/templates/text";

const templates = [
  {
    name: "text",
    Component: TextTemplate,
  },
];

// Use in your CrayonChat component

<CrayonChat
  processMessage={processMessage} // Required for NextJS setup
  responseTemplates={templates}
/>;
```

  </TabItem>
</Tabs>

## Step 4: Update your backend API to tell the LLM to use the markdown template

In your backend API, you need to tell the LLM to use the markdown template. This can be done by providing a JSON schema to the LLM. In a NextJS backend, this API may look
something like this:

<Tabs groupId='frontend-lang'>

<TabItem value="ts" label="TypeScript">

```ts
import { NextRequest } from "next/server";
import OpenAI from "openai";
import { fromOpenAICompletion, toOpenAIMessages } from "@crayonai/stream";
import { Message } from "@crayonai/react-core";

const textResponseSchema = {
  type: "object",
  description: "Use this schema to generate a markdown formatted text response",
  properties: {
    type: { const: "text" },
    text: {
      type: "string",
      description: "Markdown formatted message to be displayed to the user",
    },
  },
  required: ["type", "text"],
  additionalProperties: false,
};

export async function POST(req: NextRequest) {
  const { messages } = (await req.json()) as { messages: Message[] };
  const client = new OpenAI();
  const llmStream = await client.chat.completions.create({
    model: "gpt-4o",
    messages: toOpenAIMessages(
      messages
    ) as OpenAI.Chat.ChatCompletionMessageParam[],
    stream: true,
    response_format: textResponseSchema,
  });
  const responseStream = fromOpenAICompletion(llmStream);
  return new Response(responseStream as unknown as ReadableStream<Uint8Array>, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache, no-transform",
      Connection: "keep-alive",
    },
  });
}
```

</TabItem>

<TabItem value="js" label="JavaScript">

```js
import OpenAI from "openai";
import { fromOpenAICompletion, toOpenAIMessages } from "@crayonai/stream";

const textResponseSchema = {
  type: "object",
  description: "Use this schema to generate a markdown response",
  properties: {
    type: { const: "text" },
    text: {
      type: "string",
      description: "Markdown formatted message to be displayed to the user",
    },
  },
  required: ["type", "text"],
  additionalProperties: false,
};

export async function POST(req) {
  const { messages } = await req.json();
  const client = new OpenAI();
  const llmStream = await client.chat.completions.create({
    model: "gpt-4o",
    messages: toOpenAIMessages(messages),
    stream: true,
    response_format: textResponseSchema,
  });
  const responseStream = fromOpenAICompletion(llmStream);
  return new Response(responseStream, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache, no-transform",
      Connection: "keep-alive",
    },
  });
}
```

</TabItem>

</Tabs>

That's it! Your Crayon Chat will now use the text template for rendering text responses with Markdown support. The template will be automatically selected when the AI response specifies the "text" template type.
